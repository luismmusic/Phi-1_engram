# Microsoft Phi-1 con DeepSeek Engram: Gu√≠a Completa para Principiantes

Esta gu√≠a te ense√±ar√°, paso a paso y sin omitir detalles, c√≥mo utilizar esta versi√≥n avanzada del modelo de lenguaje **Phi-1** que incluye el m√≥dulo **Engram** de DeepSeek. Engram permite al modelo tener una "memoria" eficiente para patrones de texto repetitivos, mejorando su capacidad sin hacerlo m√°s lento.

---

## üöÄ Flujo de Trabajo en la Nube con Git (M√©todo Profesional)

Dado que est√°s trabajando en la nube (Google Colab, Kaggle), la forma m√°s r√°pida y asertiva de obtener este c√≥digo es utilizando **Git**. Git te permite "clonar" (copiar exactamente) todo este proyecto en tu entorno virtual en segundos.

### ¬øPor qu√© usar Git en lugar de subir archivos a mano?
1.  **Velocidad**: No tienes que descargar y luego subir archivos. Todo sucede directamente en la nube.
2.  **Integridad**: Te aseguras de tener todos los archivos necesarios y sus versiones correctas.
3.  **Persistencia**: Si tu entorno de nube se reinicia, solo tienes que volver a ejecutar un comando para recuperar todo.

---

## 1. Entendiendo los Archivos

En este repositorio encontrar√°s dos archivos principales de c√≥digo:

1.  **`phi1_engram.py`**: Este es el "cerebro" del modelo. Contiene todas las f√≥rmulas matem√°ticas y la estructura necesaria para que Phi-1 y Engram trabajen juntos. **No necesitas modificarlo**.
2.  **`verify_phi_engram.py`**: Es un script de prueba. Su funci√≥n es verificar que el modelo est√© bien instalado y que funcione perfectamente.

---

## 2. C√≥mo probarlo en Google Colab

Sigue estos pasos exactos y exhaustivos:

1.  **Abre Google Colab**: Ve a [colab.research.google.com](https://colab.research.google.com).
2.  **Crea un Notebook nuevo**: Haz clic en el bot√≥n "Nuevo cuaderno" (New notebook).
3.  **üî¥ PASO CR√çTICO: Activar GPU**:
    *   Sin esto, el modelo ser√° extremadamente lento y consumir√° demasiados recursos de CPU.
    *   Ve al men√∫ superior: **Entorno de ejecuci√≥n** -> **Cambiar tipo de entorno de ejecuci√≥n**.
    *   En "Acelerador de hardware", selecciona **T4 GPU** y haz clic en "Guardar".
5.  **Paso A: Instalar librer√≠as**: Copia y ejecuta este comando en la primera celda:
    ```bash
    !pip install torch transformers tokenizers numpy sympy
    ```
6.  **Paso B: Clonar el proyecto con Git**: Crea una celda nueva y ejecuta este comando (reemplaza la URL si es necesario):
    ```bash
    !git clone https://github.com/tu-usuario/Phi-1_engram.git
    %cd Phi-1_engram
    ```
    *Nota: El s√≠mbolo `!` indica a Colab que ejecute un comando del sistema, y `%cd` cambia la carpeta de trabajo.*
7.  **Paso C: Ejecutar la verificaci√≥n**: Ejecuta este comando en otra celda:
    ```bash
    !python verify_phi_engram.py
    ```
    Si ves un mensaje de √©xito con un check verde (‚úÖ), ¬°el modelo est√° listo!

---

## 3. C√≥mo usarlo en Kaggle

Kaggle es ideal para entrenamiento pesado. Sigue estas instrucciones detalladas:

1.  **Inicia sesi√≥n**: Ve a [kaggle.com](https://www.kaggle.com).
2.  **Crea un Notebook**: Haz clic en `+ Create` -> `New Notebook`.
3.  **Configura el entorno**:
    *   En el panel derecho ("Settings"), activa **Internet on**.
    *   En **Accelerator**, selecciona **GPU T4 x2**.
4.  **Descarga el c√≥digo con Git**: En la primera celda, escribe y ejecuta:
    ```bash
    !pip install torch transformers tokenizers numpy sympy
    !git clone https://github.com/tu-usuario/Phi-1_engram.git
    ```
5.  **Entra en la carpeta**:
    ```python
    import os
    os.chdir("/kaggle/working/Phi-1_engram")
    ```
6.  **Prueba el modelo**:
    ```bash
    !python verify_phi_engram.py
    ```

---

## 4. C√≥mo integrarlo con Hugging Face

Hugging Face es el repositorio central donde guardar√°s tus modelos entrenados.

1.  **Crea una cuenta**: Reg√≠strate en [huggingface.co](https://huggingface.co).
2.  **Uso desde cualquier lugar**: Una vez subido tu modelo, puedes cargarlo directamente as√≠:
    ```python
    from transformers import AutoModelForCausalLM
    # Reemplaza 'tu-usuario' por tu nombre real
    model = AutoModelForCausalLM.from_pretrained("tu-usuario/phi1-engram", trust_remote_code=True)
    ```

---

## 5. Ejemplo de "Uso B√°sico" Explicado

Aqu√≠ tienes un c√≥digo que puedes copiar y pegar. Est√° dise√±ado para ser asertivo y directo:

```python
import torch
from phi1_engram import PhiEngramConfig, PhiEngramForCausalLM
from transformers import AutoTokenizer

# 1. Configuraci√≥n: Definimos las caracter√≠sticas del modelo
config = PhiEngramConfig(
    hidden_size=256,
    num_hidden_layers=2,
    vocab_size=51200
)

# 2. Creaci√≥n: Construimos el modelo basado en la configuraci√≥n
model = PhiEngramForCausalLM(config)

# 3. Traducci√≥n: Preparamos el tokenizer oficial de Microsoft
tokenizer = AutoTokenizer.from_pretrained("microsoft/phi-1")

# 4. Procesamiento: Convertimos texto a n√∫meros y pedimos un resultado
texto = "Hola, mundo!"
inputs = tokenizer(texto, return_tensors="pt")

with torch.no_grad():
    outputs = model(**inputs)

print("¬°√âxito! El modelo gener√≥ un tensor de tama√±o:", outputs.logits.shape)
```

---

## 6. Ciclo de Trabajo Interrelacionado (El Flujo Ideal)

Para un desarrollo profesional en la nube, sigue este orden:

1.  **GIT**: Usa Git para clonar este c√≥digo en cualquier plataforma.
2.  **KAGGLE**: Entrena el modelo usando sus GPUs potentes.
3.  **HUGGING FACE**: Sube el modelo entrenado a tu Hub para guardarlo.
4.  **COLAB**: Descarga tu modelo desde Hugging Face para mostrar resultados o hacer pruebas r√°pidas.

---

## üìö Documentaci√≥n T√©cnica Detallada

Si eres un desarrollador avanzado o investigador, hemos preparado un documento exhaustivo que explica las ecuaciones matem√°ticas, la arquitectura de hashing y los detalles de integraci√≥n:

üëâ **[Consulta la Documentaci√≥n T√©cnica Completa aqu√≠](./DOCUMENTACION_TECNICA.md)**

---

## üõ†Ô∏è Pr√≥ximos Pasos T√©cnicos y Hoja de Ruta

Para avanzar de este prototipo a un modelo entrenado y optimizado, consulta nuestra **Hoja de Ruta Detallada** que incluye:
1.  **Bootstrapping**: C√≥mo cargar los pesos de Phi-1 manteniendo la memoria Engram.
2.  **Fine-tuning Estrat√©gico**: Fases de entrenamiento (Warm-up vs Conjunto).
3.  **Optimizaciones de Memoria**: Implementaci√≥n de Prefetching y CPU Offloading.
4.  **Validaci√≥n de Profundidad**: Uso de LogitLens y CKA.

üëâ **[Ver Hoja de Ruta en la Documentaci√≥n T√©cnica](./DOCUMENTACION_TECNICA.md#7-pr√≥ximos-pasos-t√©cnicos-y-hoja-de-ruta)**

---

## üéì Tutorial: Carga de Pesos y Entrenamiento

Para usar el modelo real con el conocimiento de Microsoft, sigue este flujo de trabajo completo:

### 1. Cargar Pesos Oficiales
Ejecuta el script `load_phi_engram.py`. Este script descargar√° los pesos de Phi-1 (aprox. 2.6GB) y los inyectar√° en la nueva arquitectura. Las partes de Engram se mantendr√°n nuevas (inicializadas aleatoriamente) listas para aprender.
```bash
python load_phi_engram.py
```

### 2. Entrenamiento en Dos Fases
El archivo `train_phi_engram.py` contiene la l√≥gica para entrenar el modelo correctamente:

- **Fase 1 (Warm-up)**: Se congela el "cerebro" (Phi-1) y solo se entrena la "memoria" (Engram). Esto evita que el modelo olvide lo que ya sabe mientras se adapta a la nueva estructura.
- **Fase 2 (Joint Fine-tuning)**: Se entrena todo el modelo. Engram usa un Learning Rate 5 veces m√°s alto para capturar patrones r√°pidamente, mientras que Phi-1 se ajusta suavemente.

**Para iniciar el entrenamiento**:
```bash
python train_phi_engram.py
```

### 3. Guardar y Compartir
Al finalizar, el script crear√° una carpeta `phi1-engram-trained` con todo lo necesario para subirlo a Hugging Face o usarlo en tus proyectos.

---

## üí¨ C√≥mo Hablar con el Modelo (Modo Chat)

Si quieres probar la capacidad conversacional del modelo, utiliza el script interactivo:

1.  **Ejecuta el Chat**:
    ```bash
    python chat_phi_engram.py
    ```
2.  **C√≥mo interactuar**:
    - El script te pedir√° que escribas un mensaje: `üë§ T√∫:`.
    - Escribe tu pregunta y presiona `Enter`.
    - El modelo responder√° como `ü§ñ Phi-Engram:`.
3.  **Optimizaci√≥n de Recursos**:
    El script de chat ahora utiliza **precisi√≥n reducida (FP16)** autom√°ticamente si detecta una GPU. Esto reduce el consumo de memoria RAM tanto del sistema como de la GPU a la mitad, evitando el cierre del entorno por falta de memoria.
4.  **En caso de error "Out of Memory" (OOM)**:
    Si recibes un error de memoria agotada:
    - Reinicia el entorno de ejecuci√≥n (**Entorno de ejecuci√≥n** -> **Reiniciar sesi√≥n**).
    - No ejecutes otros modelos pesados en la misma sesi√≥n.
5.  **Consejo de experto**:
    Como Phi-1 es un modelo base (no entrenado espec√≠ficamente para chat), funciona mejor si le haces preguntas directas o le pides completar frases.
